{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Data Processing\n",
    "\n",
    "#### Labels for categories:\n",
    "| Label & feature  | 1  | 2  | 3  | 4  | 5  | 6  | 7  |\n",
    "|---|---|---|---|---|---|---|---|\n",
    "| P. Zone Class  | Cold  | Warm  | Hot  |   |   |   |  \n",
    "| P. Mass Class  | Jovian  | Neptunian | Superterran  | Terran  | Subterran  | Mercurian  |  \n",
    "|  P. Compostion Class | gas  | water-gas |  rocky-water  | rocky-iron  | iron  |   |   \n",
    "| P. Atmosphere Class  | hydrogen-rich  |  metals-rich | no-atmosphere  |   |   |   |   \n",
    "\n",
    "\n",
    "For P. Habitable Class, which is our target label, we have a 3 entry output [c1, c2, c3] corresponding to [non-h, meso-, psychro-] and do a softmax on it as our network output giving something like [0.95, 0.03, 0.02] which predicts non-habitable.\n",
    "\n",
    "Our target labels then need to be as follows (I think):\n",
    "\n",
    "non-habitable: [1, 0, 0]\n",
    "\n",
    "mesoplanet: [0, 1, 0]\n",
    "\n",
    "psychroplanet: [0, 0, 1]\n",
    "\n",
    "This would work with MSE loss because it only requires that prediction $x$ has the same shape as target $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to re-label entries, assigning a numeric value to text label\n",
    "\n",
    "zone_class = {\"Cold\": 1, \"Warm\": 2, \"Hot\": 3}\n",
    "mass_class = {\"Jovian\": 1, \"Neptunian\": 2, \"Superterran\": 3, \"Terran\": 4, \"Subterran\": 5, \"Mercurian\": 6}\n",
    "composition_class = {\"gas\": 1, 'water-gas': 2, 'rocky-water': 3, 'rocky-iron': 4, 'iron': 5}\n",
    "atmosphere_class = {'hydrogen-rich': 1, 'metals-rich': 2, 'no-atmosphere': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "project_path = cwd.rsplit('\\\\', maxsplit=1)[0]\n",
    "case_num = 1\n",
    "df = pd.read_csv(os.path.join(project_path, f'data\\\\processed\\\\PHL-EC-Case{case_num}.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-label text-based values with corresponding number from above table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # re-label zone class\n",
    "# try: \n",
    "#     for label, val in zone_class.items():\n",
    "#         df.loc[df[\"P. Zone Class\"]==label, \"P. Zone Class\"] = val\n",
    "# except KeyError as exception:\n",
    "#     print(f'Excepted Key Error: {exception} - not included in this feature case')\n",
    "\n",
    "# # re-label mass class\n",
    "# try:\n",
    "#     for label, val in mass_class.items():\n",
    "#         df.loc[df[\"P. Mass Class\"]==label, \"P. Mass Class\"] = val\n",
    "# except KeyError as exception:\n",
    "#     print(f'Excepted Key Error: {exception} - not included in this feature case')\n",
    "\n",
    "# # re-label composition class\n",
    "# try:\n",
    "#     for label, val in composition_class.items():\n",
    "#         df.loc[df[\"P. Composition Class\"]==label, \"P. Composition Class\"] = val\n",
    "# except KeyError as exception:\n",
    "#     print(f'Excepted Key Error: {exception} - not included in this feature case')\n",
    "\n",
    "# # re-label atmosphere class\n",
    "# try:\n",
    "#     for label, val in atmosphere_class.items():\n",
    "#         df.loc[df[\"P. Atmosphere Class\"]==label, \"P. Atmosphere Class\"] = val\n",
    "# except KeyError as exception:\n",
    "#     print(f'Excepted Key Error: {exception} - not included in this feature case')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping \n",
    "\n",
    "The next step in data prcoessing is bootstrap aggregation, where we take the data set and produce equal number of samples from non-habitable, meso, and psychro type planets, so that the model is equally trained on all three types.\n",
    "\n",
    "We have 17 of hab_type 1 and 31 of hab_type 2, with excess of hab_type 0 (non-habitable). The paper mentions 40 times upsampling so we'll assume that means:\n",
    "$40 \\times 17 = 680$\n",
    "\n",
    "680 of each type giving us a total aggregrate dataset of 2040 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 12345 # seed to have consistent random samples\n",
    "num_samples = 680 # number of samples of each type of planet\n",
    "\n",
    "# split data into 3 dataframes based on habitability label\n",
    "df_0 = df[df[\"hab_lbl\"] == 0]\n",
    "df_1 = df[df[\"hab_lbl\"] == 1]\n",
    "df_2 = df[df[\"hab_lbl\"] == 2]\n",
    "\n",
    "# generate random indices for selectign samples\n",
    "np.random.seed(seed)\n",
    "rand_num_0 = np.random.randint(0, df_0.shape[0], size = num_samples)\n",
    "rand_num_1 = np.random.randint(0, df_1.shape[0], size = num_samples)\n",
    "rand_num_2 = np.random.randint(0, df_2.shape[0], size = num_samples)\n",
    "\n",
    "# convert to numpy arrays and sample\n",
    "agg_0 = df_0.to_numpy()[rand_num_0]\n",
    "agg_1 = df_1.to_numpy()[rand_num_1]\n",
    "agg_2 = df_2.to_numpy()[rand_num_2]\n",
    "\n",
    "# doing 80/20 train/test split we concatenate\n",
    "train = np.concatenate((agg_0[0:int(num_samples*0.8), :], agg_1[0:int(num_samples*0.8), :], agg_2[0:int(num_samples*0.8), :]))\n",
    "test = np.concatenate((agg_0[int(num_samples*0.8):, :], agg_1[int(num_samples*0.8):, :], agg_2[int(num_samples*0.8):, :]))\n",
    "\n",
    "# separating inputs and targets\n",
    "train_input, train_target1D = train[:, 1:], train[:, 0]\n",
    "test_input, test_target1D = test[:, 1:], test[:, 0]\n",
    "\n",
    "# turning targets from 1 -> [0, 1, 0], 2 -> [0, 0, 1], etc.\n",
    "train_target = np.empty((len(train_target1D), 3), dtype=int)\n",
    "train_target[np.where(train_target1D == 0), :] = [1, 0, 0]\n",
    "train_target[np.where(train_target1D == 1), :] = [0, 1, 0]\n",
    "train_target[np.where(train_target1D == 2), :] = [0, 0, 1]\n",
    "\n",
    "test_target = np.empty((len(test_target1D), 3), dtype=int)\n",
    "test_target[np.where(test_target1D == 0), :] = [1, 0, 0]\n",
    "test_target[np.where(test_target1D == 1), :] = [0, 1, 0]\n",
    "test_target[np.where(test_target1D == 2), :] = [0, 0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "[[0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " ...\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(test_target)\n",
    "np.random.shuffle(test_target)\n",
    "print(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing reshaping into subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples 1632\n",
      "subset_size 36\n",
      "iterations 45\n",
      "subset_size * iterations =  1620\n",
      "trim 12\n",
      "previous shape (1632, 45)\n",
      "new shape (45, 36, 45)\n"
     ]
    }
   ],
   "source": [
    "iterations = 45\n",
    "num_samples = train_input.shape[0]\n",
    "subset_size = int(num_samples/iterations)\n",
    "trim = num_samples - iterations*subset_size\n",
    "\n",
    "\n",
    "print('num_samples', num_samples)\n",
    "print(\"subset_size\", subset_size)\n",
    "print(\"iterations\", iterations)\n",
    "print(\"subset_size * iterations = \", subset_size*iterations)\n",
    "print(\"trim\", trim) # we have to trim off some data in order to get proper reshaping\n",
    "\n",
    "print(\"previous shape\", train_input.shape)\n",
    "train_input = np.reshape(train_input[:-trim], (iterations, subset_size, 45))\n",
    "print(\"new shape\", train_input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21e3e09130b141b1946ff20fc683d5b9b11c33d8db188a55f69210dd31c5f84c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
