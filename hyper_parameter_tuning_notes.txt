Case 1:

Identifies unhabitable with 100% accuracy, has a harder time with the 
habitable classifications.
RWNN:

Iteration: 30/30
End of iteration:        Training Loss: 0.107    Test Loss: 0.108
                         Training Accuracy: Un: 100.0%  Ps: 81.2%  Mes: 70.5%
                         Test Accuracy:     Un: 100.0%  Ps: 69.9%  Mes: 65.4%

Vanilla:

Final Results:           Training Loss: 0.106    Test Loss: 0.107
                         Training Accuracy: Un: 99.8%  Ps: 94.5%  Mes: 55.5%
                         Test Accuracy:     Un: 100.0%  Ps: 92.6%  Mes: 49.3%

Case 2:

This case performs far better when normalization of inputs is turned off.

RWNN:
Iteration: 20/20
Iteration: 20/20
End of iteration:        Training Loss: 0.030    Test Loss: 0.031
                         Training Accuracy: Un: 96.1%  Ps: 100.0%  Mes: 94.7%
                         Test Accuracy:     Un: 96.3%  Ps: 100.0%  Mes: 94.1%

Vanilla:
Final Results:           Training Loss: 0.027    Test Loss: 0.027
                         Training Accuracy: Un: 98.3%  Ps: 100.0%  Mes: 92.5%
                         Test Accuracy:     Un: 97.8%  Ps: 100.0%  Mes: 94.1%

Case 3:

Both networks struggle to differentiate psychro from meso

RWNN:
Iteration: 35/35
End of iteration:        Training Loss: 0.146    Test Loss: 0.137
                         Training Accuracy: Un: 96.6%  Ps: 0.0%  Mes: 100.0%
                         Test Accuracy:     Un: 100.0%  Ps: 0.0%  Mes: 100.0%

Vanilla:
Final Results:           Training Loss: 0.139    Test Loss: 0.140
                         Training Accuracy: Un: 99.8%  Ps: 100.0%  Mes: 3.5%
                         Test Accuracy:     Un: 100.0%  Ps: 100.0%  Mes: 2.9%

Case 4:

Similar to case 2 this performs a lot better without normalization.
It may have something to do with both case 2 & case 4 having the smallest
number of features.

RWNN:
Iteration: 35/35
End of iteration:        Training Loss: 0.069    Test Loss: 0.070
                         Training Accuracy: Un: 91.3%  Ps: 89.9%  Mes: 85.0%
                         Test Accuracy:     Un: 92.6%  Ps: 94.9%  Mes: 77.9%

Vanilla:
Final Results:           Training Loss: 0.062    Test Loss: 0.066
                         Training Accuracy: Un: 93.0%  Ps: 95.0%  Mes: 77.9%
                         Test Accuracy:     Un: 92.6%  Ps: 94.9%  Mes: 75.0%

Case 5:

RWNN:

Iteration: 35/35
End of iteration:        Training Loss: 0.067    Test Loss: 0.067
                         Training Accuracy: Un: 91.3%  Ps: 89.9%  Mes: 85.0%
                         Test Accuracy:     Un: 92.6%  Ps: 94.9%  Mes: 77.9%

Vanilla:

Final Results:           Training Loss: 0.062    Test Loss: 0.066
                         Training Accuracy: Un: 93.0%  Ps: 95.0%  Mes: 77.9%
                         Test Accuracy:     Un: 92.6%  Ps: 94.9%  Mes: 75.0%


Case 6:

Loss plateaus for both methods with essentially no difference between the two.
RWNN runs faster though because it uses less data to train per epoch.

RWNN:

Iteration: 35/35
End of iteration:        Training Loss: 0.174    Test Loss: 0.171
                         Training Accuracy: Un: 68.2%  Ps: 73.5%  Mes: 28.8%
                         Test Accuracy:     Un: 80.1%  Ps: 70.6%  Mes: 26.5%

Vanilla:

Final Results:           Training Loss: 0.169    Test Loss: 0.171
                         Training Accuracy: Un: 77.9%  Ps: 76.5%  Mes: 25.6%
                         Test Accuracy:     Un: 80.1%  Ps: 70.6%  Mes: 25.0%


Case 7:

Essentially identical to case 6. Only difference in features is that case 7
uses minimum planet mass, and case 6 uses planet mass. (Which is often the same).

RWNN:

Iteration: 35/35
End of iteration:        Training Loss: 0.175    Test Loss: 0.171
                         Training Accuracy: Un: 77.6%  Ps: 62.5%  Mes: 24.7%
                         Test Accuracy:     Un: 80.9%  Ps: 65.4%  Mes: 27.9%

Vanilla:

Final Results:           Training Loss: 0.170    Test Loss: 0.171
                         Training Accuracy: Un: 78.7%  Ps: 76.5%  Mes: 25.6%
                         Test Accuracy:     Un: 80.1%  Ps: 70.6%  Mes: 25.0%


Case 8:

RWNN:

Iteration: 35/35
End of iteration:        Training Loss: 0.122    Test Loss: 0.118
                         Training Accuracy: Un: 89.4%  Ps: 85.0%  Mes: 58.8%
                         Test Accuracy:     Un: 92.6%  Ps: 88.2%  Mes: 58.1%

Vanilla:

Final Results:           Training Loss: 0.116    Test Loss: 0.116
                         Training Accuracy: Un: 90.4%  Ps: 88.1%  Mes: 69.9%
                         Test Accuracy:     Un: 90.4%  Ps: 88.2%  Mes: 69.9%